run:
  name: imagen_base64_2b_edm
  results_dir: ${base_results_dir}/${.name}
  time_limit: "14-00:00:00"
  dependency: "singleton"

name: nemo_imagen # The name of your model
allow_tf32: True
 
trainer:
  devices: 8 # number of GPUs (0 for CPU), or list of the GPUs to use e.g. [0, 1]
  num_nodes: 8
  max_epochs: -1
  max_steps: 2500000 # precedence over max_epochs
  logger: False  # Provided by exp_manager
  use_distributed_sampler: False 
  precision: bf16 # Should be set to 16 for O1 and O2 to enable the AMP.
  accelerator: gpu
  limit_val_batches: 0
  log_every_n_steps: 5  # Interval of logging.
  num_sanity_val_steps: 10 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  enable_checkpointing: False # Provided by exp_manager
  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
  gradient_clip_val: 1.0
  benchmark: False
  enable_model_summary: True


exp_manager:
  explicit_log_dir: ${training.run.results_dir}/results
  exp_dir: null
  name: ${training.name}
  create_wandb_logger: False
  wandb_logger_kwargs: # Whether you want exp_manger to create a Wandb logger
    name: ${training.run.name}
    project: nemo_imagen
  create_tensorboard_logger: True  # Whether you want exp_manger to create a tb logger
  create_checkpoint_callback: True  # Whether you want exp_manager to create a modelcheckpoint callback
  checkpoint_callback_params:
    monitor: reduced_train_loss
    save_top_k: 5
    every_n_epochs: 0 # Save checkpoint frequency.
    every_n_train_steps: 1000 # Mutually exclusive with every_n_epochs. It is recommended to set this if training on large-scale dataset.
    filename: '${training.name}--{reduced_train_loss:.2f}-{step}-{consumed_samples}'  
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  ema:
    enable: True
    decay: 0.9999
    validate_original_weights: False
    every_n_steps: 1
    cpu_offload: False


model:
  precision: ${training.trainer.precision}
  # specify micro_batch_size, global_batch_size, and model parallelism
  # gradient accumulation will be done automatically based on data_parallel_size
  micro_batch_size: 32 # limited by GPU memory
  global_batch_size: 2048 # will use more micro batches to reach global batch size
  inductor: False
  inductor_cudagraphs: False
  unet_type: base
  channels_last: True
  unet:
    embed_dim: 512
    image_size: 64
    channels: 3
    num_res_blocks: 3
    channel_mult: [ 1, 2, 3, 4 ]
    num_attn_heads: 4
    per_head_channels: 64
    cond_dim: 2048
    attention_type: fused
    feature_pooling_type: attention
    learned_sinu_pos_emb_dim: 0
    attention_resolutions: [ 8, 16, 32 ]
    dropout: False
    use_null_token: False
    init_conv_kernel_size: 3
    gradient_checkpointing: False
    scale_shift_norm: True
    stable_attention: False
    flash_attention: True
    resblock_updown: False
    resample_with_conv: True

  # miscellaneous
  seed: 1234
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
  ddp_overlap: True # True for using PyTorch default DDP overlap. False for using Megatron's default configuration for async grad allreduce

  preconditioning_type: EDM
  preconditioning:
    loss_type: l2
    sigma_data: 0.5
    p_mean: -1.2
    p_std: 1.2
  # If want to switch to continuous DDPM training,
  # use the following config:
  # preconditioning_type: DDPM
  # preconditioning:
  #   loss_type: l2
  #   pred_objective: noise
  #   noise_schedule: cosine
  #   timesteps: 1000

  conditioning:
    embed_dim: 1024
    token_length: 128
    drop_rate: 0.1
    precached_key: embeddings_t5_xxl
    out_key: t5_text

  data:
    num_workers: 16
    train:
      dataset_path:
        - ${data_dir}/your_dataset/wdinfo.pkl
      augmentations:
        resize_smallest_side: 64
        center_crop_h_w: 64, 64
        horizontal_flip: False
      filterings: null

    webdataset:
      use_webdataset: True
      object_store: False
      infinite_sampler: False
      local_root_path: ${data_dir}/your_dataset/tarfiles_reorganized/  # each tarfile in wdinfo is relative to this
      verbose: False

  optim:
    # We need weight decay for large-scale odel
    name: fused_adam
    lr: 0.0001
    eps: 1e-8
    betas: [ 0.9, 0.999 ]
    weight_decay: 0.01
    sched:
      name: WarmupPolicy
      warmup_steps: 10000
      warmup_ratio: null