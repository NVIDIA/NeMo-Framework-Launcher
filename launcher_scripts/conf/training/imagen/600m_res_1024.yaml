run:
  name: imagen_sr1024_600m_edm
  results_dir: ${base_results_dir}/${.name}
  time_limit: "14-00:00:00"
  dependency: "singleton"

name: nemo_imagen # The name of your model
allow_tf32: True
 
trainer:
  devices: 8 # number of GPUs (0 for CPU), or list of the GPUs to use e.g. [0, 1]
  num_nodes: 8
  max_epochs: -1
  max_steps: 1250000 # precedence over max_epochs
  logger: False  # Provided by exp_manager 
  precision: bf16 # Should be set to 16 for O1 and O2 to enable the AMP.
  use_distributed_sampler: False
  accelerator: gpu
  limit_val_batches: 0
  log_every_n_steps: 5  # Interval of logging.
  num_sanity_val_steps: 10 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  enable_checkpointing: False # Provided by exp_manager
  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
  gradient_clip_val: 1.0
  benchmark: False
  enable_model_summary: True

exp_manager:
  explicit_log_dir: ${training.run.results_dir}/results
  exp_dir: null
  name: ${training.name}
  create_wandb_logger: False
  wandb_logger_kwargs: # Whether you want exp_manger to create a Wandb logger
    name: ${training.run.name}
    project: nemo_imagen
  create_tensorboard_logger: True  # Whether you want exp_manger to create a tb logger
  create_checkpoint_callback: True  # Whether you want exp_manager to create a modelcheckpoint callback
  checkpoint_callback_params:
    monitor: reduced_train_loss
    save_top_k: 5
    every_n_epochs: 0 # Save checkpoint frequency.
    every_n_train_steps: 1000 # Mutually exclusive with every_n_epochs. It is recommended to set this if training on large-scale dataset.
    filename: '${training.name}--{reduced_train_loss:.2f}-{step}-{consumed_samples}'  
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  ema:
    enable: True
    decay: 0.9999
    validate_original_weights: False
    every_n_steps: 1
    cpu_offload: False


model:
  precision: ${training.trainer.precision}
 # specify micro_batch_size, global_batch_size, and model parallelism
  # gradient accumulation will be done automatically based on data_parallel_size
  micro_batch_size: 64 # limited by GPU memory
  global_batch_size: 4096 # will use more micro batches to reach global batch size
  inductor: False
  inductor_cudagraphs: False
  channels_last: True

  unet_type: sr
  unet:
    embed_dim: 128
    image_size: 1024
    channels: 3
    channel_mult: [ 1, 2, 4, 8, 8 ]
    num_attn_heads: 8
    per_head_channels: 64
    attention_type: cross
    atnn_enabled_at: [ 0, 0, 0, 1, 1 ]
    feature_pooling_type: attention
    stride: 2
    num_resblocks: [ 2, 4, 8, 8, 8 ]
    learned_sinu_pos_emb_dim: 0
    use_null_token: False
    init_conv_kernel_size: 3
    gradient_checkpointing: False
    scale_shift_norm: True
    stable_attention: True
    flash_attention: False

  # miscellaneous
  seed: 1234
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
  ddp_overlap: True

  noise_cond_aug: True
  preconditioning_type: EDM
  preconditioning:
    loss_type: l2
    sigma_data: 0.5
    p_mean: -1.2
    p_std: 1.2
  # If want to switch to continuous DDPM training,
  # use the following config:
  # preconditioning_type: DDPM
  # preconditioning:
  #   loss_type: l2
  #   pred_objective: noise
  #   noise_schedule: cosine
  #   timesteps: 1000

  conditioning:
    embed_dim: 1024
    token_length: 128
    drop_rate: 0.1
    precached_key: embeddings_t5_xxl
    out_key: t5_text

  data:
    num_workers: 16
    train:
      dataset_path:
        - ${data_dir}/your_dataset/wdinfo.pkl
      augmentations:
        resize_smallest_side: 1024
        center_crop_h_w: 256, 256
        horizontal_flip: False
      filterings:
        resolution:
          method: larger
          value: 1024
        estimated_portion: 0.2 # Estimated % of examples left after filtering. This is use to estimate # epoch
      target_resolutions: [64, 256]

    webdataset:
      use_webdataset: True
      object_store: False
      infinite_sampler: True
      local_root_path: ${data_dir}/your_dataset/tarfiles_reorganized/  # each tarfile in wdinfo is relative to this
      verbose: False

  optim:
    # We need weight decay for large-scale odel
    name: fused_adam
    lr: 0.0001
    eps: 1e-8
    betas: [ 0.9, 0.999 ]
    weight_decay: 0.01
    sched:
      name: WarmupPolicy
      warmup_steps: 10000
      warmup_ratio: null