name: megatron_gpt
restore_from_path: null

run:
  name: train_gpt3_175b_tp4_pp6
  results_dir: ${base_results_dir}/${.name}
  time_limit: "00:30:00"
  dependency: "singleton"

trainer:
  num_nodes: 24
  devices: 8
  accelerator: gpu
  precision: bf16
  logger: false
  enable_checkpointing: false
  use_distributed_sampler: false
  max_epochs: 1
  max_steps: 200
  log_every_n_steps: 1
  val_check_interval: 8
  limit_val_batches: 1.0
  limit_test_batches: 1
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  benchmark: false
  enable_model_summary: false
  limit_train_batches: 200
  enable_progress_bar: false
  num_sanity_val_steps: 0

s3_checkpointing:
  max_write_concurrency: 10
  max_read_concurrency: 15
  chunk_size_MB: 64
  enable_async_checkpointing: false

exp_manager:
  explicit_log_dir: ${training.run.results_dir}/results
  exp_dir: null
  name: megatron_gpt
  create_wandb_logger: false
  wandb_logger_kwargs:
    project: null
    name: null
  create_neptune_logger: false
  neptune_logger_kwargs:
    project: null
    name: null
    prefix: train
    log_model_checkpoints: false
    tags: null
    description: null
  resume_if_exists: 0
  resume_ignore_no_checkpoint: true
  resume_from_checkpoint: /load_checkpoints/ckpt4000-consumed_samples=0
  create_checkpoint_callback: 0
  checkpoint_callback_params:
    dirpath: null
    monitor: val_loss
    save_top_k: 1
    mode: max
    always_save_nemo: false
    save_nemo_on_train_end: false
    filename: megatron_gpt--{val_loss:.2f}-{step}-{consumed_samples}
    model_parallel_size: 24
    async_save: false
    every_n_epochs: 0
    save_last: true
  log_step_timing: true
  create_tensorboard_logger: false
  log_global_rank_0_only: true

model:
  mcore_gpt: true
  micro_batch_size: 1
  global_batch_size: 48
  rampup_batch_size: null
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 6
  virtual_pipeline_model_parallel_size: 8
  expert_model_parallel_size: 1
  encoder_seq_length: 2048
  max_position_embeddings: 2048
  num_layers: 96
  hidden_size: 12288
  ffn_hidden_size: 49152
  num_attention_heads: 96
  init_method_std: 0.006
  use_scaled_init_method: false
  hidden_dropout: 0.0
  attention_dropout: 0.0
  ffn_dropout: 0.0
  kv_channels: null
  apply_query_key_layer_scaling: false
  normalization: layernorm1p
  layernorm_epsilon: 1.0e-05
  do_layer_norm_weight_decay: true
  make_vocab_size_divisible_by: 128
  pre_process: true
  post_process: true
  persist_layer_norm: true
  bias: true
  activation: gelu
  headscale: false
  transformer_block_type: pre_ln
  openai_gelu: false
  normalize_attention_scores: true
  position_embedding_type: learned_absolute
  rotary_percentage: 1.0
  attention_type: multihead
  share_embeddings_and_output_weights: true
  overlap_p2p_comm: true
  batch_p2p_comm: false
  seq_len_interpolation_factor: null
  num_query_groups: null
  tokenizer:
    library: sentencepiece
    type: bpe
    model: /lustre/fsr/datasets/llm/c4/tokenizers/google_c4_spm/c4_en_301_5Mexp2_spm.model
    vocab_file: null
    merge_file: null
    delimiter: null
    sentencepiece_legacy: false
  native_amp_init_scale: 4294967296
  native_amp_growth_interval: 1000
  hysteresis: 2
  fp32_residual_connection: false
  fp16_lm_cross_entropy: false
  megatron_amp_O2: true
  grad_allreduce_chunk_size_mb: 125
  grad_div_ar_fusion: true
  gradient_accumulation_fusion: true
  bias_activation_fusion: true
  bias_dropout_add_fusion: true
  masked_softmax_fusion: true
  get_attention_mask_from_fusion: true
  apply_rope_fusion: false
  seed: 11147
  resume_from_checkpoint: /load_checkpoints/ckpt4000-consumed_samples=0
  use_cpu_initialization: false
  onnx_safe: false
  apex_transformer_log_level: 30
  gradient_as_bucket_view: true
  sync_batch_comm: false
  nccl_communicator_config_path: null
  validation_param_sync_overlap: false
  fsdp: false
  fsdp_sharding_strategy: full
  fsdp_grad_reduce_dtype: 32
  fsdp_sharded_checkpoint: false
  dist_ckpt_format: zarr
  dist_ckpt_load_on_device: true
  dist_ckpt_parallel_save: false
  activations_checkpoint_granularity: null
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: null
  num_micro_batches_with_partial_activation_checkpoints: null
  activations_checkpoint_layers_per_pipeline: null
  sequence_parallel: true
  transformer_engine: true
  fp8: true
  fp8_e4m3: false
  fp8_hybrid: true
  fp8_margin: 0
  fp8_interval: 1
  fp8_amax_history_len: 1024
  fp8_amax_compute_algo: max
  reduce_amax: true
  use_emha: false
  ub_tp_comm_overlap: true
  ub_tp_comm_overlap_cfg:
    qkv_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    qkv_wgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_dgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    fc1_wgrad:
      method: bulk
      num_sm: 4
      cga_size: 2
      set_sm_margin: 0
    qkv_fprop:
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
      atomic_gemm: 0
    proj_dgrad:
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
      atomic_gemm: 0
    fc1_fprop:
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
      atomic_gemm: 0
    fc2_dgrad:
      method: ring_exchange
      num_sm: 1
      set_sm_margin: 0
      atomic_gemm: 0
    proj_fprop:
      method: pipeline
      num_sm: 24
      cga_size: 2
      num_splits: 4
      set_sm_margin: 1
      atomic_gemm: 0
      fp8_buf: 1
    fc2_fprop:
      method: pipeline
      num_sm: 4
      set_sm_margin: 1
      atomic_gemm: 1
      fp8_buf: 0
  use_flash_attention: false
  cpu_offloading: false
  cpu_offloading_num_layers: 95
  cpu_offloading_activations: true
  cpu_offloading_weights: true
  sharp: false
  enable_megatron_timers: false
  megatron_timer_kwargs:
    log_every_n_steps: 10
    log_mode: minmax
    barrier: false
  data:
    data_prefix:
      train:
        - 0.5
        - ${data_dir}/c4_en_6_c4_spm_text_document
        - 0.5
        - ${data_dir}/c4_en_7_c4_spm_text_document
      validation:
        - ${data_dir}/c4_en_validation_subset_c4_spm_text_document
      test:
        - ${data_dir}/c4_en_validation_subset_c4_spm_text_document
    index_mapping_dir: ${base_results_dir}/train_gpt3_175b_tp4_pp6
    data_impl: mmap
    mmap_bin_files: true
    splits_string: null
    seq_length: 2048
    skip_warmup: true
    num_workers: 2
    dataloader_type: single
    reset_position_ids: false
    reset_attention_mask: false
    eod_mask_loss: false
    validation_drop_last: false
    no_seqlen_plus_one_input_tokens: true
    pad_samples_to_global_batch_size: true
    shuffle_documents: false
    exchange_indices_distributed: true
    legacy_dataset: true
    delay_data_init: false
    delay_data_mmap: false
    #delay_data_init: true
    #delay_data_mmap: true
  nsys_profile:
    enabled: false
    start_step: 10
    end_step: 10
    ranks:
    - 0
    gen_shape: false
    trace:
    - nvtx
    - cuda
  optim:
    name: distributed_fused_adam
    lr: 2.0e-05
    weight_decay: 0.1
    betas:
    - 0.9
    - 0.95
    sched:
      name: CosineAnnealingExp
      warmup_steps: 132.5
      constant_steps: 0
      min_lr: 2.0e-06
      max_steps_for_lr_sched: 54300.0
    bucket_cap_mb: 200
    overlap_grad_sync: true
    overlap_param_sync: true
    contiguous_grad_buffer: true
    contiguous_param_buffer: true
    nccl_ub: false
    grad_sync_dtype: bf16
    lock_timeout: null
  gc_interval: 1000
  name: megatron_gpt_full_te_layer_autocast
  use_tp_pp_dp_mapping: false
  fp8_params: true
  enable_cuda_graph: 1
  defer_embedding_wgrad_compute: true
  use_te_rng_tracker: true
  tp_comm_overlap_ag: true
  tp_comm_overlap_rs: true
  #  custom:
  #    log_metrics: NEMO
  #    init_global_step: 0
  #    target_log_ppl: 2.69
  #    use_distributed_checkpointing: 1
  #    run_warmup_on_synth_data: 1
  #    reset_fp8_stats_after_warmup: 1
  #    pre_validate: 0
  #    override_zero_consumed_samples: 0
  #    force_success_status: 1
  #    warmup_train_steps: 2
  #    warmup_validation_steps: 2
  #    extend_run_evals: 0
  #    disable_nemo_logs: true
  #proxy_gbs: 3072
  #is_proxy_run: true
