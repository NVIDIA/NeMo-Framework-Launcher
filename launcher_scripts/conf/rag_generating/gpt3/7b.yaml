hydra:
  searchpath:
    - file:///opt/NeMo/examples/nlp/language_modeling/conf

run:
  name: ${.eval_name}_${.model_train_name}
  time_limit: "4:00:00"
  dependency: "singleton"
  nodes: 1
  ntasks_per_node: 1
  eval_name: rag_generating
  model_train_name: gpt3
  results_dir: ${base_results_dir}/${.name}

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: False # logger provided by exp_manager
  precision: 'bf16-mixed'
  use_distributed_sampler: False

indexing:
  embedder:
    model_type: bert
    model_path: null
    embed_batch_size: 128
  data:
    data_path: null
    chunk_size: 256
    chunk_overlap: 10
  index_path: null

generating:
  llm:
    model_type: gpt
    model_path: null
  query: null
  inference:
    greedy: False # Whether or not to use sampling ; use greedy decoding otherwise
    top_k: 0  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
    top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
    temperature: 1.0 # sampling temperature
    add_BOS: True # add the bos token at the begining of the prompt
    tokens_to_generate: 500 # The minimum length of the sequence to be generated.
    all_probs: False  # whether return the log prob for all the tokens in vocab
    repetition_penalty: 1.2  # The parameter for repetition penalty. 1.0 means no penalty.
    min_tokens_to_generate: 0  # The minimum length of the sequence to be generated.
    compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False
    end_strings: ["<|endoftext|>"]  # generation will stop when one of these tokens is generated