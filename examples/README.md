### Example scripts for pre-train and finetuning 
These scripts run a recommended config for GPT, LLAMA2, Nemotron pretraining, and finetuning for various model sizes on A100, H100. For example, for GPT3 pretrain the following folders provide sample scripts.

- [a100](https://github.com/NVIDIA/NeMo-Megatron-Launcher/tree/master/examples/training/gpt/a100)
: Scripts to run GPT pretraining on NVIDIA A100, in bf16 data type

- [h100](https://github.com/NVIDIA/NeMo-Megatron-Launcher/tree/master/examples/training/gpt/h100)
: Scripts to run GPT pretraining for NVIDIA H100, in fp8 data type

#### Setup
1. To run these scripts, you must have access to the nemo bignlp container (like nvcr.io/nvidia/nemo:24.01.framework)
     - If you don't have access, please signup at https://developer.nvidia.com/nemo-framework-open-beta
       
2. Update the following bash variables in the example run scripts:
     - ``` NEMO_MEGATRON_LAUNCHER_DIR ``` : the directory of where this repository is located

     - ``` DATA_DIR ``` : the directory of the dataset used for pretraining, by default this is ``` NEMO_MEGATRON_LAUNCHER_DIR/data ```

3. Enter your cluster enviroment settings at 
  [config.yaml](https://github.com/NVIDIA/NeMo-Megatron-Launcher/blob/master/launcher_scripts/conf/config.yaml)
    
    For bcm type clusters update the job name, partition, and account at [bcm.yaml]( https://github.com/NVIDIA/NeMo-Megatron-Launcher/blob/master/launcher_scripts/conf/cluster/bcm.yaml)

4. For testing performance with synthetic data on an interactive node, you need to add the following options to your bash script:
    ```
            cluster_type=interactive \
            ++training.cluster_type=BCP \
            training.model.data.data_impl="mock" \
            training.model.data.data_prefix=[]
    ```
    
For further details see [General Configuration](https://docs.nvidia.com/nemo-framework/user-guide/latest/modelguide/usingautoconfigurator.html#general-configuration) 

#### Results
Results are by default stored at ``` NEMO_MEGATRON_LAUNCHER_DIR/results/<experiment_name> ``` with the following structure:

- ``` NEMO_MEGATRON_LAUNCHER_DIR/results/<experiment_name>/<experiment_name>.yaml ``` : The config of the pretrained model
- ``` NEMO_MEGATRON_LAUNCHER_DIR/results/<experiment_name>/<jobname>_<experiment_name>.sh ``` : The autogenerated .sh file that was run
- ``` NEMO_MEGATRON_LAUNCHER_DIR/results/<experiment_name>/results/ ``` : Directory contained per rank logs, and tensorboard data.

For further details see [Interpreting the Results](https://docs.nvidia.com/nemo-framework/user-guide/latest/modelguide/usingautoconfigurator.html#interpreting-the-results) 

### Benchmark performance numbers (pretraining)

- Please refer to [MLCommons Training results](https://mlcommons.org/benchmarks/training/) for performance of GPT3-175B pre-training on large scale H100 systems. 
- To calculate Model TFLOPs, please see Appendix A in [paper](https://arxiv.org/pdf/2205.05198.pdf).

  
| Model | GPU | Precision | #-GPUs | Global Batch <br> Size | Model TFLOPs <br> per GPU | Est. time to train in days <br> (1T tokens, 1K GPUs) |
| ---      | ---      | ---      | ---      | ---      | ---      | ---     |
| GPT3-5B | H100 | BF16 | 64 | 2048 | 510 | 0.7  |
| GPT3-5B | H100 | FP8 | 64 | 2048 | 738 | 0.5  |
| GPT3-20B | H100 | BF16 | 64 | 256 | 466 | 3.1  |
| GPT3-20B | H100 | FP8 | 64 | 256 | 660 | 2.2  |
| LLAMA2-7B | H100 | BF16 | 8 | 128 | 511 | 1.0  |
| LLAMA2-7B | H100 | FP8 | 8 | 128 | 694 | 0.8  |
| LLAMA2-13B | H100 | BF16 | 16 | 128 | 471 | 2.1  |
| LLAMA2-13B | H100 | FP8 | 16 | 128 | 674 | 1.5  |
| LLAMA2-70B | H100 | BF16 | 64 | 128 | 467 | 10.8  |
| LLAMA2-70B | H100 | FP8 | 64 | 128 | 708 | 7.1  |
| Nemotron-8B | H100 | BF16 | 8 | 32 | 453 | 1.3  |
| Nemotron-8B  | H100 | FP8 | 8 | 32 | 593 | 1.0  |
| Nemotron-22B | H100 | BF16 | 16 | 32 | 395 | 3.7  |
| Nemotron-22B  | H100 | FP8 | 16 | 32 | 499 | 3.0  |


### Benchmark performance numbers (finetuning)

- The following table provides performance benchmarking of LLAMA2 models with SFT (supervised fine-tuning), and LoRA (Low-rank adaptors) on H100.
- For fine-tuning, we use packed input dataset, and the inputs are packed to 4096 tokens.

| Model | Mode | GPU | Precision | #-GPUs | Global Batch <br> Size | Model TFLOPs <br> per GPU | Est. time in mins <br> (run 40M tokens) |
| ---      | ----- | ---      | ---      | ---      | ---      | ---      | ---     |
| LLAMA2-7B | SFT | H100 | BF16 | 8 | 8 | 430 | 8 |
| LLAMA2-13B | SFT | H100 | BF16 | 8 | 8 | 467 | 14 |
| LLAMA2-13B | SFT | H100 | FP8 | 8 | 8 | 600 | 11 |
| LLAMA2-7B | LoRA | H100 | BF16 | 8 | 8 | 471 | 5 |
| LLAMA2-13B | LoRA | H100 | BF16 | 8 | 8 | 507 | 9 |
| LLAMA2-13B | LoRA | H100 | FP8 | 8 | 8 | 623 | 7 |
